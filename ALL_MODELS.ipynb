{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9253a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  // THIS NOTEBOOK CONTAINS ALL THE DIFFERENT MODEL ARCHITECTURES USED IN THE ASSIGNMENT\n",
    "\n",
    "#  // MODELS ANNOTATED WITH V2 HAVE THE SAME ARCHITECTURE, BUT ARE TRAINED WITH A DIFFERENT OPTIMIZER (ADAM INSTEAD OF RMSPROP)\n",
    "\n",
    "#  // MODELS ANNOTATED WITH V3 ARE WITH AN ADAM OPTIMIZER BUT WITH A LEARNING RATE OF 1E-4, AND MODELS ANNOTATED WITH V4 ARE TRAINED WITH A LEARNING RATE OF 1E-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdbe84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  // CREATING THE CNN CLASS, ALONG WITH THE DIFFERENT LAYERS FOR VISUALIZATION LATER\n",
    "\n",
    "class MODEL1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MODEL1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding='same')\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding='same')\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5, padding='same')\n",
    "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 7, padding='same')\n",
    "        self.batchnorm4 = nn.BatchNorm2d(256)\n",
    "        self.fc1 = nn.Linear(36864, 128) ###\n",
    "        self.batchnorm5 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(.2)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.batchnorm6 = nn.BatchNorm1d(32)\n",
    "        self.dropout2 = nn.Dropout(.4)\n",
    "        self.fc3 = nn.Linear(32, 7)    #  // OUTPUT IS ONE OF THE SEVEN EMOTIONS IN THE DATASET    \n",
    "       \n",
    " \n",
    "    def forward(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = self.batchnorm1(F.leaky_relu(self.conv1(x), negative_slope=0.01))\n",
    "        \n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = self.batchnorm2(F.leaky_relu(self.conv2(x), negative_slope=0.01))\n",
    "        \n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = self.batchnorm3(F.leaky_relu(self.conv3(x), negative_slope=0.01))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 4\n",
    "        x = self.batchnorm4(F.leaky_relu(self.conv4(x), negative_slope=0.01))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // OUT\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.batchnorm5(F.relu(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.batchnorm6(F.relu(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.softmax(self.fc3(x), dim=1)       \n",
    "        return x\n",
    "    \n",
    "    def getLayer1(self, x, training=False):\n",
    "        \n",
    "        x = self.batchnorm1(F.leaky_relu(self.conv1(x), negative_slope=0.01))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def getLayer2(self, x, training=False):\n",
    "        \n",
    "        x = self.batchnorm1(F.leaky_relu(self.conv1(x), negative_slope=0.01))\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = self.batchnorm2(F.leaky_relu(self.conv2(x), negative_slope=0.01))\n",
    "        \n",
    "        return x\n",
    "    def getLayer3(self, x, training=False):\n",
    "        \n",
    "        x = self.batchnorm1(F.leaky_relu(self.conv1(x), negative_slope=0.01))\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = self.batchnorm2(F.leaky_relu(self.conv2(x), negative_slope=0.01))\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = self.batchnorm3(F.leaky_relu(self.conv3(x), negative_slope=0.01))\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "    \n",
    "    def getLayer4(self, x, training=False):\n",
    "        \n",
    "        x = self.batchnorm1(F.leaky_relu(self.conv1(x), negative_slope=0.01))\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = self.batchnorm2(F.leaky_relu(self.conv2(x), negative_slope=0.01))\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = self.batchnorm3(F.leaky_relu(self.conv3(x), negative_slope=0.01))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        \n",
    "        x = self.batchnorm4(F.leaky_relu(self.conv4(x), negative_slope=0.01))\n",
    "        x = self.pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf7d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  // CREATING THE CNN CLASS, ALONG WITH THE DIFFERENT LAYERS FOR VISUALIZATION LATER\n",
    "\n",
    "class MODEL2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MODEL2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding='same')\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding='same')\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5, padding='same')\n",
    "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 7, padding='same')\n",
    "        self.batchnorm4 = nn.BatchNorm2d(256)\n",
    "        self.fc1 = nn.Linear(36864, 128) ###\n",
    "        self.batchnorm5 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(.2)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.batchnorm6 = nn.BatchNorm1d(32)\n",
    "        self.dropout2 = nn.Dropout(.4)\n",
    "        self.fc3 = nn.Linear(32, 7)        \n",
    "       \n",
    " \n",
    "    def forward(self, x, training=False):\n",
    "        #  // LAYER 1 \n",
    "        x = F.leaky_relu(self.batchnorm1(self.conv1(x)), negative_slope=0.01)\n",
    "        \n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.batchnorm2(self.conv2(x)), negative_slope=0.01)\n",
    "        \n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = F.leaky_relu(self.batchnorm3(self.conv3(x)), negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 4\n",
    "        x = F.leaky_relu(self.batchnorm4(self.conv4(x)), negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // OUT\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.batchnorm5(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.batchnorm6(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.softmax(self.fc3(x), dim=1)       \n",
    "        return x\n",
    "    \n",
    "    def getLayer1(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.batchnorm1(self.conv1(x)), negative_slope=0.01)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def getLayer2(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.batchnorm1(self.conv1(x)), negative_slope=0.01)\n",
    "        \n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.batchnorm2(self.conv2(x)), negative_slope=0.01)\n",
    "        \n",
    "        return x\n",
    "    def getLayer3(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.batchnorm1(self.conv1(x)), negative_slope=0.01)\n",
    "        \n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.batchnorm2(self.conv2(x)), negative_slope=0.01)\n",
    "        \n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = F.leaky_relu(self.batchnorm3(self.conv3(x)), negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "    \n",
    "    def getLayer4(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.batchnorm1(self.conv1(x)), negative_slope=0.01)\n",
    "       \n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.batchnorm2(self.conv2(x)), negative_slope=0.01)\n",
    "        \n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = F.leaky_relu(self.batchnorm3(self.conv3(x)), negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 4\n",
    "        x = F.leaky_relu(self.batchnorm4(self.conv4(x)), negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51905dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  // CREATING THE CNN CLASS, ALONG WITH THE DIFFERENT LAYERS FOR VISUALIZATION LATER\n",
    "\n",
    "\n",
    "class MODEL3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MODEL3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding='same')\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding='same')\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5, padding='same')\n",
    "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 7, padding='same')\n",
    "        self.batchnorm4 = nn.BatchNorm2d(256)\n",
    "        self.fc1 = nn.Linear(2304, 128) \n",
    "        self.batchnorm5 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(.2)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.batchnorm6 = nn.BatchNorm1d(32)\n",
    "        self.fc3 = nn.Linear(32, 7)        \n",
    "       \n",
    " \n",
    "    def forward(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = self.batchnorm1(F.leaky_relu(self.conv1(x), negative_slope=0.005))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = self.batchnorm2(F.leaky_relu(self.conv2(x), negative_slope=0.005))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = self.batchnorm3(F.leaky_relu(self.conv3(x), negative_slope=0.005))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 4\n",
    "        x = self.batchnorm4(F.leaky_relu(self.conv4(x), negative_slope=0.005))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // OUT\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.batchnorm5(F.leaky_relu(self.fc1(x), negative_slope=0.005))\n",
    "        x = self.batchnorm6(F.leaky_relu(self.fc2(x), negative_slope=0.005))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.softmax(self.fc3(x), dim=1)       \n",
    "        return x\n",
    "    \n",
    "    def getLayer1(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = self.batchnorm1(F.leaky_relu(self.conv1(x), negative_slope=0.005))\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "    \n",
    "    def getLayer2(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = self.batchnorm1(F.leaky_relu(self.conv1(x), negative_slope=0.005))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = self.batchnorm2(F.leaky_relu(self.conv2(x), negative_slope=0.005))\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "    def getLayer3(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = self.batchnorm1(F.leaky_relu(self.conv1(x), negative_slope=0.005))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = self.batchnorm2(F.leaky_relu(self.conv2(x), negative_slope=0.005))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = self.batchnorm3(F.leaky_relu(self.conv3(x), negative_slope=0.005))\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "    \n",
    "    def getLayer4(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = self.batchnorm1(F.leaky_relu(self.conv1(x), negative_slope=0.005))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = self.batchnorm2(F.leaky_relu(self.conv2(x), negative_slope=0.005))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = self.batchnorm3(F.leaky_relu(self.conv3(x), negative_slope=0.005))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 4\n",
    "        x = self.batchnorm4(F.leaky_relu(self.conv4(x), negative_slope=0.005))\n",
    "        x = self.pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b971bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  // CREATING THE CNN CLASS, ALONG WITH THE DIFFERENT LAYERS FOR VISUALIZATION LATER\n",
    "\n",
    "class MODEL4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MODEL4, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding='same')\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding='same')\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5, padding='same')\n",
    "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 7, padding='same')\n",
    "        self.batchnorm4 = nn.BatchNorm2d(256)\n",
    "        self.fc1 = nn.Linear(2304, 128) \n",
    "        self.batchnorm5 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(.2)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.batchnorm6 = nn.BatchNorm1d(32)\n",
    "        self.dropout2 = nn.Dropout(.4)\n",
    "        self.fc3 = nn.Linear(32, 7)        \n",
    "       \n",
    " \n",
    "    def forward(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.batchnorm1(self.conv1(x)), negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.batchnorm2(self.conv2(x)), negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = F.leaky_relu(self.batchnorm3(self.conv3(x)), negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 4\n",
    "        x = F.leaky_relu(self.batchnorm4(self.conv4(x)), negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // OUT\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.batchnorm5(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.batchnorm6(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.softmax(self.fc3(x), dim=1)       \n",
    "        return x\n",
    "    \n",
    "    def getLayer1(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.batchnorm1(self.conv1(x)), negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "    \n",
    "    def getLayer2(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.batchnorm1(self.conv1(x)), negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.batchnorm2(self.conv2(x)), negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "    def getLayer3(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.batchnorm1(self.conv1(x)), negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.batchnorm2(self.conv2(x)), negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = F.leaky_relu(self.batchnorm3(self.conv3(x)), negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "    \n",
    "    def getLayer4(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.batchnorm1(self.conv1(x)), negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.batchnorm2(self.conv2(x)), negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = F.leaky_relu(self.batchnorm3(self.conv3(x)), negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 4\n",
    "        x = F.leaky_relu(self.batchnorm4(self.conv4(x)), negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78ccf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  // CREATING THE CNN CLASS, ALONG WITH THE DIFFERENT LAYERS FOR VISUALIZATION LATER\n",
    "\n",
    "class MODEL5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MODEL5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(.5)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 32, 3)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(32)\n",
    "        self.conv5 = nn.Conv2d(32, 32, 3)\n",
    "        self.batchnorm5 = nn.BatchNorm2d(32)\n",
    "        self.conv6 = nn.Conv2d(32, 32, 3)\n",
    "        self.batchnorm6 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout2 = nn.Dropout(.6)\n",
    "        self.fc1 = nn.Linear(1568, 128)\n",
    "        self.batchnorm7 = nn.BatchNorm1d(128)\n",
    "        self.dropout3 = nn.Dropout(.5)\n",
    "        self.fc2 = nn.Linear(128, 7)   \n",
    "       \n",
    " \n",
    "    def forward(self, x, training=False):\n",
    "        \n",
    "        #  // LAYER 1\n",
    "        x = F.relu(self.batchnorm1(self.conv1(x)))\n",
    "        x = F.relu(self.batchnorm2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        \n",
    "        x = F.relu(self.batchnorm3(self.conv3(x)))\n",
    "        x = F.relu(self.batchnorm4(self.conv4(x)))\n",
    "        x = F.relu(self.batchnorm5(self.conv5(x)))\n",
    "        x = F.relu(self.batchnorm6(self.conv6(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x) \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.batchnorm7(self.fc1(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def getLayer1(self, x, training=False):\n",
    "        \n",
    "        #  // LAYER 1\n",
    "        \n",
    "        x = F.relu(self.batchnorm1(self.conv1(x)))\n",
    "        x = F.relu(self.batchnorm2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        return x\n",
    "    \n",
    "    def getLayer2(self, x, training=False):\n",
    "        \n",
    "        #  // LAYER 1\n",
    "        \n",
    "        x = F.relu(self.batchnorm1(self.conv1(x)))\n",
    "        x = F.relu(self.batchnorm2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        \n",
    "        x = F.relu(self.batchnorm3(self.conv3(x)))\n",
    "        x = F.relu(self.batchnorm4(self.conv4(x)))\n",
    "        x = F.relu(self.batchnorm5(self.conv5(x)))\n",
    "        x = F.relu(self.batchnorm6(self.conv6(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6086d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  // CREATING THE CNN CLASS, ALONG WITH THE DIFFERENT LAYERS FOR VISUALIZATION LATER\n",
    "\n",
    "class MODEL6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MODEL6, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 5, padding='same')\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 5, padding='same')\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(.4)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding='same')\n",
    "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding='same')\n",
    "        self.batchnorm4 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout2 = nn.Dropout(.4)\n",
    "        self.conv5 = nn.Conv2d(128, 256, 3, padding='same')\n",
    "        self.batchnorm5 = nn.BatchNorm2d(256)\n",
    "        self.conv6 = nn.Conv2d(256, 256, 3, padding='same')\n",
    "        self.batchnorm6 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout3 = nn.Dropout(.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.batchnorm7 = nn.BatchNorm1d(128)\n",
    "        self.dropout4 = nn.Dropout(.6)\n",
    "        self.fc2 = nn.Linear(128, 7)   \n",
    "       \n",
    "    def forward(self, x):\n",
    "        #  // LAYER 1\n",
    "        x = self.batchnorm1(F.elu(self.conv1(x)))\n",
    "        x = self.batchnorm2(F.elu(self.conv2(x))) \n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = self.batchnorm3(F.elu(self.conv3(x)))\n",
    "        x = self.batchnorm4(F.elu(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = self.batchnorm5(F.elu(self.conv5(x)))       \n",
    "        x = self.batchnorm6(F.elu(self.conv6(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        #  // OUT\n",
    "        x = torch.flatten(x, 1)        \n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = self.batchnorm7(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def getLayer1(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = self.batchnorm1(F.elu(self.conv1(x)))\n",
    "        x = self.batchnorm2(F.elu(self.conv2(x))) \n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        return x\n",
    "    \n",
    "    def getLayer2(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = self.batchnorm1(F.elu(self.conv1(x)))\n",
    "        x = self.batchnorm2(F.elu(self.conv2(x))) \n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = self.batchnorm3(F.elu(self.conv3(x)))\n",
    "        x = self.batchnorm4(F.elu(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return x\n",
    "    \n",
    "    def getLayer3(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = self.batchnorm1(F.elu(self.conv1(x)))\n",
    "        x = self.batchnorm2(F.elu(self.conv2(x))) \n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = self.batchnorm3(F.elu(self.conv3(x)))\n",
    "        x = self.batchnorm4(F.elu(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = self.batchnorm5(F.elu(self.conv5(x)))       \n",
    "        x = self.batchnorm6(F.elu(self.conv6(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1419afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  // CREATING THE CNN CLASS, ALONG WITH THE DIFFERENT LAYERS FOR VISUALIZATION LATER\n",
    "\n",
    "class MODEL7(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MODEL7, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, padding='same')\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, padding='same')\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, 7, padding='same')\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, 7, padding='same')\n",
    "        self.batchnorm2 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(36864, 128) \n",
    "        self.batchnorm3 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(.2)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(32)\n",
    "        self.dropout2 = nn.Dropout(.2)\n",
    "        self.fc3 = nn.Linear(32, 7)        \n",
    "       \n",
    " \n",
    "    def forward(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.01)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.conv2(x), negative_slope=0.01)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = F.leaky_relu(self.conv3(x), negative_slope=0.01)\n",
    "        \n",
    "        #  // LAYER 4\n",
    "        x = F.leaky_relu(self.conv4(x), negative_slope=0.01)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // OUT\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.batchnorm3(F.relu(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.batchnorm4(F.relu(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.softmax(self.fc3(x), dim=1)       \n",
    "        return x\n",
    "    \n",
    "    def getLayer1(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.01)\n",
    "        return x\n",
    "    \n",
    "    def getLayer2(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.01)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.conv2(x), negative_slope=0.01)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.pool(x)       \n",
    "        return x\n",
    "    \n",
    "    def getLayer3(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.01)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.conv2(x), negative_slope=0.01)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = F.leaky_relu(self.conv3(x), negative_slope=0.01)\n",
    "        return x\n",
    "    \n",
    "    def getLayer4(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.01)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.conv2(x), negative_slope=0.01)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = F.leaky_relu(self.conv3(x), negative_slope=0.01)\n",
    "        \n",
    "        #  // LAYER 4\n",
    "        x = F.leaky_relu(self.conv4(x), negative_slope=0.01)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  // CREATING THE CNN CLASS, ALONG WITH THE DIFFERENT LAYERS FOR VISUALIZATION LATER\n",
    "\n",
    "class MODEL8(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MODEL8, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding='same')\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, padding='same')\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, 7, padding='same')\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, 9, padding='same')\n",
    "        self.batchnorm2 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(36864, 128) \n",
    "        self.batchnorm3 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(.2)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(.2)\n",
    "        self.fc3 = nn.Linear(64, 7)        \n",
    "       \n",
    " \n",
    "    def forward(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.01)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.conv2(x), negative_slope=0.01)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = F.leaky_relu(self.conv3(x), negative_slope=0.01)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        #  // LAYER 4\n",
    "        x = F.leaky_relu(self.conv4(x), negative_slope=0.01)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        #  // OUT\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.batchnorm3(F.relu(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.batchnorm4(F.relu(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.softmax(self.fc3(x), dim=1)       \n",
    "        return x\n",
    "    \n",
    "    def getLayer1(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.01)\n",
    "        x = self.dropout1(x)\n",
    "        return x\n",
    "    \n",
    "    def getLayer2(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.01)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.conv2(x), negative_slope=0.01)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout1(x)     \n",
    "        return x\n",
    "    \n",
    "    def getLayer3(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.01)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.conv2(x), negative_slope=0.01)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = F.leaky_relu(self.conv3(x), negative_slope=0.01)\n",
    "        x = self.dropout1(x)\n",
    "        return x\n",
    "    \n",
    "    def getLayer4(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.01)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.conv2(x), negative_slope=0.01)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = F.leaky_relu(self.conv3(x), negative_slope=0.01)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        #  // LAYER 4\n",
    "        x = F.leaky_relu(self.conv4(x), negative_slope=0.01)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af96d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  // CREATING THE CNN CLASS, ALONG WITH THE DIFFERENT LAYERS FOR VISUALIZATION LATER\n",
    "\n",
    "class MODEL9(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MODEL9, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding='same')\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, padding='same')\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, 5, padding='same')   \n",
    "        self.conv4 = nn.Conv2d(64, 64, 5, padding='same')\n",
    "        self.conv5 = nn.Conv2d(64, 64, 5, padding='same')\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(3, 3)\n",
    "\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(64, 128, 5, padding='same')   \n",
    "        self.conv7 = nn.Conv2d(128, 128, 5, padding='same')\n",
    "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        \n",
    "        self.conv8 = nn.Conv2d(128, 256, 5, padding='same')   \n",
    "        self.conv9 = nn.Conv2d(256, 256, 5, padding='same')\n",
    "        self.conv10 = nn.Conv2d(256, 256, 5, padding='same')\n",
    "        self.batchnorm4 = nn.BatchNorm2d(256)\n",
    "        self.pool2 = nn.MaxPool2d(3, 3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(6400, 256) \n",
    "        self.batchnorm5 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(.2)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.batchnorm6 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(.2)\n",
    "        self.fc3 = nn.Linear(64, 7)        \n",
    "       \n",
    " \n",
    "    def forward(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv2(x), negative_slope=0.005)\n",
    "        x = self.batchnorm1(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.conv3(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv4(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv5(x), negative_slope=0.005)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = F.leaky_relu(self.conv6(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv7(x), negative_slope=0.005)\n",
    "        x = self.batchnorm3(x)\n",
    "\n",
    "        \n",
    "        #  // LAYER 4\n",
    "        x = F.leaky_relu(self.conv8(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv9(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv10(x), negative_slope=0.005)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        #  // OUT\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.batchnorm5(F.relu(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.batchnorm6(F.relu(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.softmax(self.fc3(x), dim=1)       \n",
    "        return x\n",
    "    \n",
    "    def getLayer1(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv2(x), negative_slope=0.005)\n",
    "        x = self.batchnorm1(x)\n",
    "        return x\n",
    "    \n",
    "    def getLayer2(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv2(x), negative_slope=0.005)\n",
    "        x = self.batchnorm1(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.conv3(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv4(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv5(x), negative_slope=0.005)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.pool1(x)\n",
    "        return x\n",
    "    \n",
    "    def getLayer3(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv2(x), negative_slope=0.005)\n",
    "        x = self.batchnorm1(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.conv3(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv4(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv5(x), negative_slope=0.005)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = F.leaky_relu(self.conv6(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv7(x), negative_slope=0.005)\n",
    "        x = self.batchnorm3(x)\n",
    "        return x\n",
    "    \n",
    "    def getLayer4(self, x, training=False):\n",
    "        #  // LAYER 1\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv2(x), negative_slope=0.005)\n",
    "        x = self.batchnorm1(x)\n",
    "        \n",
    "        #  // LAYER 2\n",
    "        x = F.leaky_relu(self.conv3(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv4(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv5(x), negative_slope=0.005)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        \n",
    "        #  // LAYER 3\n",
    "        x = F.leaky_relu(self.conv6(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv7(x), negative_slope=0.005)\n",
    "        x = self.batchnorm3(x)\n",
    "\n",
    "        \n",
    "        #  // LAYER 4\n",
    "        x = F.leaky_relu(self.conv8(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv9(x), negative_slope=0.005)\n",
    "        x = F.leaky_relu(self.conv10(x), negative_slope=0.005)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.pool2(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
